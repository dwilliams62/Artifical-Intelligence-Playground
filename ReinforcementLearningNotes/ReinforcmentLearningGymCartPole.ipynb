{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning\n",
        "- Reinforcement learning is one of the three main types of ways machine learning algorithms can be trained to learn about their environment. An agent interacts with an enviroment and through trial and error learns about the environment they are in as well as how to maximize the rewards it recieves. Reinforcement learning is an inceredibly useful practice to understand, as it is the only way we can realistically train a lot of models, such as self-driving cars and those used in the field of robotics."
      ],
      "metadata": {
        "id": "52XCf5kW1zyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In order to learn more about implementing Reinforcement Learning, the python library Gymnasium offers a playground to test models by putting them in predefined environments and allowing users to expirment with their algorithms in a variety of use cases. This notebook will be my notes on the article linked at the top written by Arun Nanda, as well as notes on Reinforcement learning as a whole when specifically using Gymnasium to test the algorithms out."
      ],
      "metadata": {
        "id": "8J_ydWiz5zd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Gymnasium?\n",
        "- As Arun Nanda puts it, \"Gymnasium is an open-source Python library designed to support the development of RL algorithms.\" This includes providing an environment for the algorithms to be ran in, from simple environments like a racing game, to complex environments that mimic  real life scenarios. It also provides a streamlined API to allow for easy collaboration with others, as well as the ability to create custom environments and have them posted for others to use through the API. From beginners to long-time experts of RL algorithms, Gymnasium has something for everyone. Gymnasium can be used to do everything you'd need with RL algorithms, such as telling the environment what action the agent took, keeping track of the environment and what rewards are gleamed, training the model, and testing out how the model is doing performance wise.\n",
        "\n"
      ],
      "metadata": {
        "id": "j_qVkxQ625K7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up Gymnasium\n",
        "- Gymnasium requires a bit of a specific setup, as it has been tuned for specific verious of the dependencies it requires, such as NumPy and PyTorch. Therefore, performing any Gymnasium work in a virtual environment or something similar (like this cloud based notebook) is a must. To install Gymnasium and all the correct dependencies, simply use the command:"
      ],
      "metadata": {
        "id": "J6pJbBqcycRn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_XVN7rbkV4H",
        "outputId": "cf0d40af-65bf-481e-e8ec-9810a22b3f7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To make sure the right packages have been imported and are ready for use in the notebook, follow the simple group of import statements shown below."
      ],
      "metadata": {
        "id": "b-cKHupUyl5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as distributions\n",
        "import numpy as np\n",
        "import gymnasium as gym"
      ],
      "metadata": {
        "id": "vU2s6VuGyvaG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- If you'd like to view all the environments avaliable to you, simply loop through the registry keys avaliable in the environment, as shown here:"
      ],
      "metadata": {
        "id": "KVD-AhPby1LC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "for i in gym.envs.registry.keys():\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvRsVbC0y3uM",
        "outputId": "a82cafac-a480-4b93-f5c4-3169f2bd601b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CartPole-v0\n",
            "CartPole-v1\n",
            "MountainCar-v0\n",
            "MountainCarContinuous-v0\n",
            "Pendulum-v1\n",
            "Acrobot-v1\n",
            "phys2d/CartPole-v0\n",
            "phys2d/CartPole-v1\n",
            "phys2d/Pendulum-v0\n",
            "LunarLander-v3\n",
            "LunarLanderContinuous-v3\n",
            "BipedalWalker-v3\n",
            "BipedalWalkerHardcore-v3\n",
            "CarRacing-v3\n",
            "Blackjack-v1\n",
            "FrozenLake-v1\n",
            "FrozenLake8x8-v1\n",
            "CliffWalking-v0\n",
            "Taxi-v3\n",
            "tabular/Blackjack-v0\n",
            "tabular/CliffWalking-v0\n",
            "Reacher-v2\n",
            "Reacher-v4\n",
            "Reacher-v5\n",
            "Pusher-v2\n",
            "Pusher-v4\n",
            "Pusher-v5\n",
            "InvertedPendulum-v2\n",
            "InvertedPendulum-v4\n",
            "InvertedPendulum-v5\n",
            "InvertedDoublePendulum-v2\n",
            "InvertedDoublePendulum-v4\n",
            "InvertedDoublePendulum-v5\n",
            "HalfCheetah-v2\n",
            "HalfCheetah-v3\n",
            "HalfCheetah-v4\n",
            "HalfCheetah-v5\n",
            "Hopper-v2\n",
            "Hopper-v3\n",
            "Hopper-v4\n",
            "Hopper-v5\n",
            "Swimmer-v2\n",
            "Swimmer-v3\n",
            "Swimmer-v4\n",
            "Swimmer-v5\n",
            "Walker2d-v2\n",
            "Walker2d-v3\n",
            "Walker2d-v4\n",
            "Walker2d-v5\n",
            "Ant-v2\n",
            "Ant-v3\n",
            "Ant-v4\n",
            "Ant-v5\n",
            "Humanoid-v2\n",
            "Humanoid-v3\n",
            "Humanoid-v4\n",
            "Humanoid-v5\n",
            "HumanoidStandup-v2\n",
            "HumanoidStandup-v4\n",
            "HumanoidStandup-v5\n",
            "GymV21Environment-v0\n",
            "GymV26Environment-v0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There are quite a bit of environments avaliable as of writing, at least 40 of them, including the duplicate versions of the same environment. Ensure that the version you're using is the right one for you, and that it is consistent in your application."
      ],
      "metadata": {
        "id": "bpaVZoH16maD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing an Environment\n",
        "- To see what one can do in Gymnasium simply, we'll go over some of the simple commands avaliable in Gymnasium. This includes checking the observation space, checking an example observation of the environment, and checking the action space. But firstly, what is an environment, and what is the observation and action spaces?\n"
      ],
      "metadata": {
        "id": "wpzB2ejw63Ax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The environment is what the RL algorithm's agent uses to determine how it will proceed, by interacting with the environment and determining how much of a reward it got for interacting in that specific way, and updating its policy accordingly. Some keywords, defined clearly, are\n",
        " - Environment: The world or region or space in which the agent interacts in a series of timesteps. At each timestep, the agent performs an action and recieves a reward before the Environment decides what the next state will look like.\n",
        " - State: \"A mathematical state of the current configuration of the Environment\" (Nanda). For example, a state could include for a self-driving car system something like the current velocity of the car and how far down the pedal is being pressed. These are only two small pieces of the overall state. A terminal state is one that does not lead to another state; That state is the end of the road.\n",
        " - Agent: The name for the algorithm that is performing the actions. The Agent will be the one with the final say on how to act at each time step after taking into account the policy.\n",
        " - Observation: A mathematical view of what the Agent sees when it views the Environment, for example, the readings of a sensor or an image of the current state decoded into vectors.\n",
        " - Action: The final decision the Agent makes on what to do at the next time step. Influences the Environment and the reward gleamed.\n",
        " - Reward: What the Environment tells the Agent about the results of it's Action. Can be good or bad depending on various factors in the Environment.\n",
        " - Return: The expected reward over future time steps. This is usually discounted to make it so that further out predictions are less credited. Can be tuned.\n",
        " - Policy: The instructions that dictate the way the Agent acts at each time step given all the information provided to the Agent at each time step. The policy is typically represented as a Probability Matrix that maps each state to an action. As Nanda puts it, \"Given a finite set of m possible states and n possible actions, element P$_m$$_n$ in the matrix denotes the probability of taking action an in the state s$_m$.\" More on the math heavy side of this later.\n",
        " - Episode: The time steps in a series that starts from the initial random state and ends at the terminal state once the Agent has reached it."
      ],
      "metadata": {
        "id": "0QxJ-Nuy9ND1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To begin seeing what an environment is, we'll look at some data returned by the Gymnasium environment, using the CartPole environment for simplicity. Create a new CartPole-v1 environment using the gym import as such:"
      ],
      "metadata": {
        "id": "TWmoRYxly_oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make('CartPole-v1')"
      ],
      "metadata": {
        "id": "3BpK3JbhzAUJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now that we have the environment, we can check the observation space as so:"
      ],
      "metadata": {
        "id": "EzaO_AdGzDV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"observation space: \", env.observation_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3d9I9nHzE1g",
        "outputId": "e2c69036-a18a-46bb-81c0-0bc405677a09"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation space:  Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- What exactly are we looking at? Well, the Observation Space is the space of all possible states that the environment could be in. This is also the format in which this data will be stored and how the Agent will interact with it. It is typically represented as an object of datatype Box, which describes the parameters of the observations using an ndarray. What we just viewed shows us the bounds of each dimension, of which CartPole has four. In the two arrays, there are four entries each. Each entry in each position corresponds with the entry in the same position in the opposite array. For example, -4.8 and 4.8 go together to form the bounds for one of the variables. The four variables avaliable in CartPole are, in this order, Cart Position, Cart Velocity, Pole Angle, and Pole Angular Velocity."
      ],
      "metadata": {
        "id": "f2YfTvO1GRx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can see an example of the observation provided to the Agent by using the following command:\n"
      ],
      "metadata": {
        "id": "vAAmtFNdz8Xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "observation, info = env.reset()\n",
        "print(\"observation: \", observation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxb0-grBz-q3",
        "outputId": "6686a934-4774-414f-ed8b-596614033b69"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation:  [-0.01472102 -0.00383868 -0.02615577 -0.04561709]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- These four elements are the four variables discussed previously, with values filled out for how the environment currently is at this state.  "
      ],
      "metadata": {
        "id": "fcc46SWgJHRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The action space, now, is the space of all actions the Agent can take and the format they can be relayed in. This format can be viewed with the following command:\n"
      ],
      "metadata": {
        "id": "iF3TPWKR0CeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"action space: \", env.action_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11PpcVoy0Eym",
        "outputId": "be797b9b-8935-49f0-c9fd-2904bbc968b7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action space:  Discrete(2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The reason the model returned \"Discrete(2)\" is because there are only 2 actions the Agent can take at any given time step. This includes a 0, which is pushing the cart to the left, and a 1, which is pushing the cart to the right. When the algorithm first begins, it does not know which button does which, and will discover for itself through trial and error what it does."
      ],
      "metadata": {
        "id": "_NNFOQ7sJzsA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Reinforcement Learning Agent\n",
        "- Now that we know a lot of the background knowledge needed, we can continue on to finally getting down and implementing an Agent with an Algorithm in an Environment. We will continue using the CartPole Environment for simplicity state. Begin by making sure the environment has been created:"
      ],
      "metadata": {
        "id": "kiqWQNX9KnVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')"
      ],
      "metadata": {
        "id": "PqaEGuWT0Sf_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- You can use then use the \"reset\" command to reset the environment as well as provide it with a starting seed, if we wanted to reproduce the same initial state. The seed needs to be passed to the dependencies being used as well. This can be done as follows:"
      ],
      "metadata": {
        "id": "u6Pmhgwb0XKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1234\n",
        "\n",
        "env.reset(seed=SEED);"
      ],
      "metadata": {
        "id": "CAR40pN-0bFg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL4XXkzD0duL",
        "outputId": "5c59d9c0-830a-4b84-c4e4-c48290971a80"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x796e05a01370>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Agent with a Policy\n",
        "- The most important part of the RL Agent is the policy that controls the Action it takes for each State provided by the Environment. This is what fills in the Probability Matrix discussed previously. By creating an approriate policy, the rewards the Agent gleams at the end of each Episode can be maximized and the time taken to reach the predefined terminal state will be minimized."
      ],
      "metadata": {
        "id": "tQFalO8rLxus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There are many different ways to define the Policy the Agent uses. The simplest is to allow the Agent to make a random decision at each time step without taking any factors into consideration. This is not only slow and leads to a very long run time, but the rewards that could have been gleamed are typically much lower than what they could have been. As such, using information gleaned from the Environment's current State and influencing the policy accordingly is a much better way of going about it."
      ],
      "metadata": {
        "id": "mIK9cJAKM6jS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Nanda mentions various methods of policy optimization, such as the Bellman Equations and Proximal Policy Optimization (PPO), but settles on Policy Gradients for the tutorial. Some other methods that may be worth looking into include Evolutionary Strategies which are modeled off of natural evolution and Trust Region Policy Optimization (TRPO) which works by limiting the amount of change between each policy to a \"safe\" amount.\n",
        "\n"
      ],
      "metadata": {
        "id": "QFqE6geJNa9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing a Simple Policy Gradient\n",
        "- Policy Gradients are \"a reinforcement learning technique that uses gradient descent to optimize a policy's parameters.\" The goal, as it is for many reinforcement learning techniques, is to maximize the long-term reward. Policy Gradients are good for handling continuous states and actions, incorporating domain-specific knowledge with ease, and are guaranteed to converge eventually to at least show off a locally optimal policy. However, there are some downsides, including them being hard to use in discrete systems, not leading to globally optimal parameters, and they are difficult to use in environments without policys. Why you'd be using the Policy Gradient algorithm without a Policy, I'm not quite sure.\n",
        "\n"
      ],
      "metadata": {
        "id": "qTW-cDh80hHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Despite the CartPole environment being discrete, we will be using the Policy Gradient algorithm to implement the environment for it. To begin setting up the Agent to use the Policy Gradient, we will create a neural network for the imlpementation of the policy, calculate the rewards and loss through functions we define as well as the probability of each action, then update the policy with all this information using backpropagation techniques.\n"
      ],
      "metadata": {
        "id": "WVpa8CyjQLAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The neural network to implement the Policy Gradient will have input dimensions equivalent to the dimensions of the observation space of the environment, a single hidden layer with 64 neurons, and output dimensions equivalent to the dimensions of the action space of the environment. Therefore, the Policy Gradient takes an observation space and maps it to an action space using the 64 neurons that it will train with each iteration until the mapping reaches a satisfactory state (terminal state)."
      ],
      "metadata": {
        "id": "uA91cc8E-KZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We define the class \"PolicyNetwok\", which is initialized with the parameters that are\n",
        " - the number of dimensions in the input space (observation space)\n",
        " - the number of neurons in the hidden layer (in this case, 64)\n",
        " - the number of dimensions in the output space (action space)\n",
        " - the dropout, which is the fraction of data that is randomly zeroed to ensure that one neuron is not used more than the rest of them\n",
        "\n"
      ],
      "metadata": {
        "id": "gAoW0eWR-xDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We then define the \"Forward\" function, which defines the flow of data throughout the neural network. In this case,\n",
        " - data x (which is the observation space data) is passed first through the first layer, which maps observation space data to the neural network's only hidden layer.\n",
        " - That hidden layer data is then subject to dropout, where random values are set to 0.\n",
        " - Then, ReLU (Rectified Linear Unit) is applied, which introduces non-linearity by taking all negative values and turning them into 0 (which supposedly helps the network learn complex patterns).\n",
        " - Finally, the data, after having been transformed by the dropout and ReLU, is mapped from the hidden layer to the action space and returned."
      ],
      "metadata": {
        "id": "9HYwSNNv_ufT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "y8rKcSzb0huA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Next, we need to define a function to calculate the returns at each timestep. The returns are the culmative sum of the rewards from the beginning to the current timestep.\n",
        " - In this function, we'll pass in two parameters: the rewards that were created by the environment and the discount factor that we're currently using.\n",
        " - We'll start by defining an empty array for the returns, which we will be calculating, and a number to hold the current return, which starts at 0.\n",
        " - Then we loop through all the rewards in reverse order, to start with the oldest one first. We will apply the discount factor at each step to the current return before adding it to the reward. Then, the current return is added to the beginning of the returns list, to ensure the list remains in the correct order (since we're looping through the rewards in reverse order).\n",
        " - Once we have the returns list complete, we turn them into a PyTorch Tensor for use with the neural network, and normalize each data value to ensure that the mean is 0, standard deviation is 1, and no numbers get too big. This also help ensure smooth and stable training.\n"
      ],
      "metadata": {
        "id": "rcuBgORQ0kde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_stepwise_returns(rewards, discount_factor):\n",
        "    returns = []\n",
        "    R = 0\n",
        "\n",
        "    for r in reversed(rewards):\n",
        "        R = r + R * discount_factor\n",
        "        returns.insert(0, R)\n",
        "    returns = torch.tensor(returns)\n",
        "    normalized_returns = (returns - returns.mean()) / returns.std()\n",
        "    return normalized_returns"
      ],
      "metadata": {
        "id": "ArO_iCvl0l8N"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Then, we need to implement a function that will allow us to simulate the forward pass of the Agent. This will be where the Agent uses the current policy and explores the environment until reaching a Terminal State, in which all the data is compiled and used to retrain the model.\n",
        " - To begin, we define the forward_pass function with three parameters: the current environment we're using, the policy that we're using, and the current discount factor of the test we're performing. (Line 1)\n",
        " - We then start prepping the collection of relevant data by creating empty containers. This includes an array to hold the log probabilities of each action, an array to hold the rewards the Agent collects, a flag to tell us if the terminal state has been reached or not, the culmunative sum of all the rewards for this pass (the return), and a confirmation that the policy is set to training mode. The environment is then reset to its initial state and testing can begin. (Lines 2-8)\n",
        " - So long as our terminal state flag \"done\" has not been flagged, the program will continue to run in a loop. Each of these times looped is a timestep. (Line 10)\n",
        " - The observation starts with the data from the reset of the environment, which is then convented into a PyTorch tensor for use with the neural network, and \"unsqueezed\", which adds a batch dimension to be used with the neural network input (not 100% on this one). (Line 11)\n",
        " - We then get a prediction of the action the Agent will take by running it through the Policy, which returns action \"logits\", or raw data about the probability of the actions that has not been normalized. The data is then normalized by the softmax function, which ensures all the action's probabilities add up to 1 and maintains a relative magnitude between each prediction. (Line 12-13)\n",
        " - From here we take the list of actions and their probabilities that the Agent will take them and create a categorical probability distribution, which basically says \"you have this chance to pick this item\" based off the probabilities we fed it. And then we go into the distribution we created and sample it, which will pick one of the actions randomly. The reason we don't just always pick the most likely action is to encourage exploring new avenues every now and then and facilitate learning. We then calculate the log probability of the sampled action, for use later when re-evaluating the policy. (Lines 14-16)\n",
        " - Then we finally apply our selected Action to the Environment and collect the data from interacting with it, including a new observation, the latest reward, if the environment reached a terminal state or ended early due to time (truncated), and any additional info that might be outputted by the environment. (Line 18)\n",
        " - Finally, we check if terminated or truncated are true, and if one is we signify to our flag \"done\" that we are done. We add the log probability of the selected action to the array of log probabilities of all actions so far, add the reward to the array of all rewards, and increase our episode_return variable by the amount of the reward. Should done by true, we continue on, otherwise the while loop goes back through it again and the Agent interacts with the environment once more. (Lines 20-23)\n",
        " - Once the loop is done we concatenate all the log_prob_actions into one tensor for easier use later on down the line and use the function we previously defined to calculate the discounted rewards for each step using the rewards list before returning the data on the episode's return, the stepwise returns at each timestep, and log probability of all the actions took (Lines 24-28)"
      ],
      "metadata": {
        "id": "3PUhbXj20oVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(env, policy, discount_factor):\n",
        "    log_prob_actions = []\n",
        "    rewards = []\n",
        "    done = False\n",
        "    episode_return = 0\n",
        "\n",
        "    policy.train()\n",
        "    observation, info = env.reset()\n",
        "\n",
        "    while not done:\n",
        "        observation = torch.FloatTensor(observation).unsqueeze(0)\n",
        "        action_pred = policy(observation)\n",
        "        action_prob = F.softmax(action_pred, dim = -1)\n",
        "        dist = distributions.Categorical(action_prob)\n",
        "        action = dist.sample()\n",
        "        log_prob_action = dist.log_prob(action)\n",
        "\n",
        "        observation, reward, terminated, truncated, info = env.step(action.item())\n",
        "\n",
        "        done = terminated or truncated\n",
        "        log_prob_actions.append(log_prob_action)\n",
        "        rewards.append(reward)\n",
        "        episode_return += reward\n",
        "\n",
        "    log_prob_actions = torch.cat(log_prob_actions)\n",
        "    stepwise_returns = calculate_stepwise_returns(rewards, discount_factor)\n",
        "\n",
        "    return episode_return, stepwise_returns, log_prob_actions"
      ],
      "metadata": {
        "id": "gCY4TCqF0qJs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We then need to use the information gleaned from the latest episode to calculate the loss, which is, as Nanda puts it, \"the quantity on which we apply gradient descent.\" This is essentially what we expect the return to be over the episode, which is calculated by multiplying each timestep's return by the probability of the action it took at that timestep and summing all the relevant values, and then flipping the sign as this would typically be used to find the minimum value, but in Reinforcement Learning we want to find the maximum reward. This value of the loss we calculated here, which acts as essentially how far off the model was, is then passed back to be used for updating the Policy.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iieQsSDu0s3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(stepwise_returns, log_prob_actions):\n",
        "    loss = -(stepwise_returns * log_prob_actions).sum()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "I8oug_FN0w-j"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We utilize the function to calculate the loss when we update the policy, which we will need another function for. This one will accept the stepwise returns and log_prob_actions we calculated from the forward pass, as well as an optimizer that will act upon the neural network's parameters. To that end, we need the loss to act upon the nearal network based off how the Agent did in the latest pass. We detatch the stepwise returns to ensure the data is treated as a fixed value and not part of the graph during backpropogation, then we calculate the loss with the function we previously declared. Then, we clear any previous data leftover in the optimizer to ensure we're only using data from this episode, compute the gradients of the loss with respect to our network's parameters, and then update the network parameters in respect to the learning rate we've set. The learning rate will be set in the Main. The loss is then converted into a Python Scalar from a PyTorch tensor to allow for easier logging and is then returned by the function. This function is the definition of the Policy Gradient Method."
      ],
      "metadata": {
        "id": "a1N9UfXj0z3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_policy(stepwise_returns, log_prob_actions, optimizer):\n",
        "    stepwise_returns = stepwise_returns.detach()\n",
        "    loss = calculate_loss(stepwise_returns, log_prob_actions)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "ne75Vvwi01aP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now we get to put it all together in the main function, utilizing all the functions we've written thus far to train and evaluate our Policy using an Agent. First, we need to define the hyperparameters, which are the parameters that we define and tune ourselves outside of the model that still impact how the model runs. Our hyperparameters include:\n",
        " - The max amount of epochs, or iterations, or episodes that the agent will work through before calling it quits.\n",
        " - The discount factor, which is passed to our policy when calculating returns.\n",
        " - The number of trials we wait before re-evaluating how the Agent is currently doing. If the average after these is above our reward threshold, the model has done what we wanted it to do and the trials end early.\n",
        " - The reward threshold, or how good the reward should be before the model has reached a satisfactory level.\n",
        " - The print interval, or how many trials we wait before printing out the current results.\n",
        " - The input dimension amount, which is determined by the environment we are currently using.\n",
        " - The hidden dimension amount, which controls how many neurons are in the hidden layer of our neural network.\n",
        " - The output dimension amount, which is determined by the environment we are currently using.\n",
        " - The dropout amount, which is the fraction of data that is randomly 0'd to ensure that we do not overly rely on the same path for too long.\n",
        " - The learning rate, which impacts how much influence the gradients have on the neural network's parameters and how quickly they will change. A value that should be just right, not too low or too high.\n",
        "- Once we have our hyperparameters set and have initialized an empty array to hold our episode returns, we instantiate our policy using the dimension information we just defined and set up an optimizer using Adam. From there, we loop through a set loop a number of times equal to our max epochs, or less if we reach our reward threshold sooner. The loop goes as follows:\n",
        "  - The forward pass is conducted, using the environment, the policy, and the discount factors to fill out the current episode's return, the stepwise returns, and the log probability of the actions. (Line 20)\n",
        "  - The policy is updated using the stepwise returns, the log probability of the actions taken, and the optimizer we defined in the main using Adam. The loss that is returned is discarded by the use of the _ variable. (Line 21)\n",
        "  - The current episode return is appended to the full list, and the mean of the last n trials is calculated by the syntax \"[-N_TRIALS:]\", where the [x:y] means that it will take a slice of the list from element x to element y inclusivly. The negative in front of N_TRIALS means to start counting from the opposite end, so a negative index here means N_TRIALS amount from the end of the list. Combine it all together and this is the last N_TRIALS amount of entries from the episode returns averaged together. (Line 23-24)\n",
        "  - If the episode we're currently on is one that needs to be printed based off of our print_interval, we print out the current episode and the means rewards. If the mean return is greater than our reward threshold, we complete the simulation and stop the loop early after printing out how many episodes it took. (Line 26-30)"
      ],
      "metadata": {
        "id": "H4E1cU7A03lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    MAX_EPOCHS = 500\n",
        "    DISCOUNT_FACTOR = 0.99\n",
        "    N_TRIALS = 25\n",
        "    REWARD_THRESHOLD = 475\n",
        "    PRINT_INTERVAL = 10\n",
        "    INPUT_DIM = env.observation_space.shape[0]\n",
        "    HIDDEN_DIM = 128\n",
        "    OUTPUT_DIM = env.action_space.n\n",
        "    DROPOUT = 0.5\n",
        "\n",
        "    episode_returns = []\n",
        "\n",
        "    policy = PolicyNetwork(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT)\n",
        "\n",
        "    LEARNING_RATE = 0.01\n",
        "    optimizer = optim.Adam(policy.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "    for episode in range(1, MAX_EPOCHS+1):\n",
        "        episode_return, stepwise_returns, log_prob_actions = forward_pass(env, policy, DISCOUNT_FACTOR)\n",
        "        _ = update_policy(stepwise_returns, log_prob_actions, optimizer)\n",
        "\n",
        "        episode_returns.append(episode_return)\n",
        "        mean_episode_return = np.mean(episode_returns[-N_TRIALS:])\n",
        "\n",
        "        if episode % PRINT_INTERVAL == 0:\n",
        "            print(f'| Episode: {episode:3} | Mean Rewards: {mean_episode_return:5.1f} |')\n",
        "\n",
        "        if mean_episode_return >= REWARD_THRESHOLD:\n",
        "            print(f'Reached reward threshold in {episode} episodes')\n",
        "            break"
      ],
      "metadata": {
        "id": "cgMAZhuU05Nk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the program. If the training doesn't converge, run it again, or change the SEED values."
      ],
      "metadata": {
        "id": "WwvIh1Lm0_uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjvWa7jF1AL5",
        "outputId": "411aad08-29dc-475c-8d1e-b3aa2b614e33"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Episode:  10 | Mean Rewards:  24.4 |\n",
            "| Episode:  20 | Mean Rewards:  18.1 |\n",
            "| Episode:  30 | Mean Rewards:  14.4 |\n",
            "| Episode:  40 | Mean Rewards:  14.3 |\n",
            "| Episode:  50 | Mean Rewards:  23.8 |\n",
            "| Episode:  60 | Mean Rewards:  44.1 |\n",
            "| Episode:  70 | Mean Rewards:  71.1 |\n",
            "| Episode:  80 | Mean Rewards: 113.9 |\n",
            "| Episode:  90 | Mean Rewards: 129.1 |\n",
            "| Episode: 100 | Mean Rewards: 150.7 |\n",
            "| Episode: 110 | Mean Rewards: 212.0 |\n",
            "| Episode: 120 | Mean Rewards: 249.0 |\n",
            "| Episode: 130 | Mean Rewards: 334.4 |\n",
            "| Episode: 140 | Mean Rewards: 377.7 |\n",
            "| Episode: 150 | Mean Rewards: 332.3 |\n",
            "| Episode: 160 | Mean Rewards: 181.5 |\n",
            "| Episode: 170 | Mean Rewards:  84.9 |\n",
            "| Episode: 180 | Mean Rewards:  56.8 |\n",
            "| Episode: 190 | Mean Rewards:  50.5 |\n",
            "| Episode: 200 | Mean Rewards:  47.8 |\n",
            "| Episode: 210 | Mean Rewards:  47.8 |\n",
            "| Episode: 220 | Mean Rewards:  54.0 |\n",
            "| Episode: 230 | Mean Rewards:  73.9 |\n",
            "| Episode: 240 | Mean Rewards: 108.8 |\n",
            "| Episode: 250 | Mean Rewards: 137.7 |\n",
            "| Episode: 260 | Mean Rewards: 146.7 |\n",
            "| Episode: 270 | Mean Rewards: 124.0 |\n",
            "| Episode: 280 | Mean Rewards: 105.2 |\n",
            "| Episode: 290 | Mean Rewards: 118.6 |\n",
            "| Episode: 300 | Mean Rewards: 193.5 |\n",
            "| Episode: 310 | Mean Rewards: 316.8 |\n",
            "| Episode: 320 | Mean Rewards: 372.2 |\n",
            "| Episode: 330 | Mean Rewards: 387.4 |\n",
            "| Episode: 340 | Mean Rewards: 427.1 |\n",
            "Reached reward threshold in 347 episodes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "- The following noteboook was created with the help of the lovely article linked here:\n",
        "https://www.datacamp.com/tutorial/reinforcement-learning-with-gymnasium."
      ],
      "metadata": {
        "id": "hBPUlkzlJTei"
      }
    }
  ]
}