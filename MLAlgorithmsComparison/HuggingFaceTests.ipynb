{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this notebook is to help facilitate the testing of various models to try and create the most optimal machine learning algorithm possible for dividing scholarly articles into the BioPsychoSocial, as explained and refined by Dr Karl Maier. The overall goal here being to help separate the articles into their somewhat distinct categories for better finding of related content in the academic field. The models in this notebook are facilitated by Hugging Face, which hosts a library of models with a standardized way of interacting with each."
      ],
      "metadata": {
        "id": "Y9gdhPSqpAbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-Shot Classification Attempt\n"
      ],
      "metadata": {
        "id": "4OXIK1t7o12h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Begin by performing a pip install to gather the required libraries if they do not exist already"
      ],
      "metadata": {
        "id": "DsoDU4j3p6ql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers pandas"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gFfSUsVZ2arv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The following piece of code was an inital test that was used just to see the Hugging Face pipeline in action. It had horrible accuracy compared to what the real answer should be, lol."
      ],
      "metadata": {
        "id": "1c28sQGZ-M0b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZY7TY-DTdef"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load zero-shot classification pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Text to classify\n",
        "text = \"This study examines the impact of climate change on marine biodiversity.\"\n",
        "\n",
        "# Labels with descriptions\n",
        "labels = [\n",
        "    \"Biophysical: Related to the physical and biological aspects of the environment and living organisms.\",\n",
        "    \"Social: Pertaining to human society, interactions, and social structures.\",\n",
        "    \"Psychological: Concerned with mental processes and behavior.\"\n",
        "]\n",
        "\n",
        "# Perform classification\n",
        "result = classifier(text, candidate_labels=labels, multi_label=False)\n",
        "\n",
        "# Output results\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- From there, the information I wanted to test the model on needed to be uploaded, hence the file.upload found directly after this. The articles used for testing here can be found on the Zotero directory created by Dr Karl Maier, which was ran through some python data prep to ensure no bad values were included beforehand."
      ],
      "metadata": {
        "id": "M3zr_RMz-SLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AtXIbWSj8rPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- From there we run the first values found in the csv through the model and compare the model's predictions to what the actual category is it belongs. For this test, it was quite innacurate."
      ],
      "metadata": {
        "id": "Vj09e89t-c-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Step 2: Load the CSV file\n",
        "file_path = \"processed_articles_sanitized.csv\"  # Replace with your actual file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Extract the first row of data after the header\n",
        "first_row = data.iloc[0]\n",
        "title = first_row['title']\n",
        "abstract = first_row['abstract']\n",
        "true_category = first_row['category']\n",
        "\n",
        "# Combine title and abstract for classification with improved formatting\n",
        "text_to_classify = f\"Title - {title}. Abstract - {abstract}\"\n",
        "\n",
        "# Step 3: Define labels with descriptions\n",
        "labels = [\n",
        "    \"Biophysical - Involves the physical systems and processes of the body, such as genetics, cellular functions, brain activity, and how they interact with environmental factors to influence health, behavior, and overall functioning.\",\n",
        "    \"Social - Encompasses the influence of relationships, cultural norms, socioeconomic structures, and societal dynamics on individual and collective well-being, focusing on the interconnectedness of people within communities and broader social systems.\",\n",
        "    \"Psychological - Refers to the mental and emotional processes that shape perception, decision-making, resilience, and responses to challenges, emphasizing internal experiences like thoughts, emotions, and coping strategies.\"\n",
        "]\n",
        "\n",
        "# Step 4: Load the zero-shot classification pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Step 5: Perform classification on the first row's text\n",
        "result = classifier(text_to_classify, candidate_labels=labels, multi_label=False)\n",
        "\n",
        "# Step 6: Extract the predicted label with the highest score\n",
        "predicted_label_full = result['labels'][0]  # Highest scoring label\n",
        "predicted_label = predicted_label_full.split(\" - \")[0]  # Extract the short label (e.g., \"Biophysical\")\n",
        "\n",
        "# Print results\n",
        "print(\"Classification Results:\")\n",
        "print(f\"Text to classify: {text_to_classify}\")\n",
        "print(f\"True category: {true_category}\")\n",
        "print(f\"Predicted category: {predicted_label}\")\n",
        "print(f\"Prediction correct? {predicted_label.lower() == true_category.lower()}\")\n",
        "\n",
        "# Print confidence scores for all labels\n",
        "print(\"\\nModel Confidence Scores:\")\n",
        "for label, score in zip(result['labels'], result['scores']):\n",
        "    print(f\"{label.split(' - ')[0]}: {score:.4f}\")"
      ],
      "metadata": {
        "id": "dMcdOcI99HXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- From there we updated what information was being recorded and presented to the user, changing from just a simple right or wrong to using the normalized confidence gap and the weighted accuracy score. The weighted accuracy score isn't the best one to use here considering some articles could reasonably belong to multiple categories, but it showed some more insight into how the model was doing. Which for this one test was terrible."
      ],
      "metadata": {
        "id": "XtGcZvUR-tbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = \"processed_articles_sanitized.csv\"  # Replace with your actual file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Extract the first row of data after the header\n",
        "first_row = data.iloc[0]\n",
        "title = first_row['title']\n",
        "abstract = first_row['abstract']\n",
        "true_category = first_row['category']\n",
        "\n",
        "# Combine title and abstract for classification\n",
        "text_to_classify = f\"Title - {title}. Abstract - {abstract}\"\n",
        "\n",
        "# Define labels with descriptions\n",
        "labels = [\n",
        "    \"Biophysical - Involves the physical systems and processes of the body, such as genetics, cellular functions, brain activity, and how they interact with environmental factors to influence health, behavior, and overall functioning.\",\n",
        "    \"Social - Encompasses the influence of relationships, cultural norms, socioeconomic structures, and societal dynamics on individual and collective well-being, focusing on the interconnectedness of people within communities and broader social systems.\",\n",
        "    \"Psychological - Refers to the mental and emotional processes that shape perception, decision-making, resilience, and responses to challenges, emphasizing internal experiences like thoughts, emotions, and coping strategies.\"\n",
        "]\n",
        "\n",
        "# Load the zero-shot classification pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Perform classification\n",
        "result = classifier(text_to_classify, candidate_labels=labels, multi_label=False)\n",
        "\n",
        "# Extract scores and labels\n",
        "scores = result['scores']\n",
        "predicted_label_full = result['labels'][0]  # Highest scoring label\n",
        "predicted_label = predicted_label_full.split(\" - \")[0]  # Short label (e.g., \"Biophysical\")\n",
        "\n",
        "# Find the index of the true category in the result labels\n",
        "true_index = [label.split(\" - \")[0] for label in result['labels']].index(true_category)\n",
        "\n",
        "# Calculate Normalized Confidence Gap and Weighted Accuracy Score\n",
        "n_labels = len(scores)\n",
        "random_baseline = 1 / n_labels\n",
        "\n",
        "# Normalize scores\n",
        "normalized_scores = [score / sum(scores) for score in scores]\n",
        "correct_score = normalized_scores[true_index]\n",
        "gap_random = correct_score - random_baseline\n",
        "\n",
        "# Weighted Accuracy Score\n",
        "rank = sorted(normalized_scores, reverse=True).index(correct_score) + 1\n",
        "weighted_accuracy = 1.0 if rank == 1 else 0.5 if rank == 2 else 0.0\n",
        "\n",
        "# Output results\n",
        "print(f\"Predicted Label: {predicted_label}\")\n",
        "print(f\"Correct Label: {true_category}\")\n",
        "print(\"\\nModel Confidence Scores:\")\n",
        "for label, score in zip(result['labels'], result['scores']):\n",
        "    print(f\"{label.split(' - ')[0]}: {score:.4f}\")\n",
        "print(f\"\\nNormalized Confidence Gap: {gap_random:.4f}\")\n",
        "print(f\"Weighted Accuracy Score: {weighted_accuracy:.1f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "VyGmAY31_st3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Once the foundation was set up, the program was changed to instead run every single article through the model and average the scores for all articles as well as all articles of each category. This started out extremely slow until the notebook was switched to use a google gpu instead of my device's specs. The results were interesting in that almost all the tests averaged out to be just about the same as if the model had randomly guessed. Which was disheartening to see."
      ],
      "metadata": {
        "id": "VmYGoyGAE1o9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "import time\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = \"processed_articles_sanitized.csv\"  # Replace with your actual file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Define labels with descriptions\n",
        "labels = [\n",
        "    \"Biophysical - Involves the physical systems and processes of the body, such as genetics, cellular functions, brain activity, and how they interact with environmental factors to influence health, behavior, and overall functioning.\",\n",
        "    \"Social - Encompasses the influence of relationships, cultural norms, socioeconomic structures, and societal dynamics on individual and collective well-being, focusing on the interconnectedness of people within communities and broader social systems.\",\n",
        "    \"Psychological - Refers to the mental and emotional processes that shape perception, decision-making, resilience, and responses to challenges, emphasizing internal experiences like thoughts, emotions, and coping strategies.\"\n",
        "]\n",
        "\n",
        "# Load the zero-shot classification pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Variables for aggregating statistics\n",
        "category_stats = {label.split(\" - \")[0]: {\"gap_sum\": 0, \"accuracy_sum\": 0, \"count\": 0} for label in labels}\n",
        "overall_gap_sum = 0\n",
        "overall_accuracy_sum = 0\n",
        "total_articles = 0\n",
        "\n",
        "# Start timing the execution\n",
        "start_time = time.time()\n",
        "\n",
        "# Define a batch size\n",
        "batch_size = 100\n",
        "\n",
        "# Process articles in batches\n",
        "for batch_start in range(0, len(data), batch_size):\n",
        "    elapsed_time = time.time() - start_time\n",
        "    if elapsed_time > 720:\n",
        "        print(\"\\nTime limit reached. Stopping further processing...\")\n",
        "        break\n",
        "\n",
        "    batch_end = min(batch_start + batch_size, len(data))\n",
        "    batch = data.iloc[batch_start:batch_end]\n",
        "\n",
        "    # Combine titles and abstracts\n",
        "    texts_to_classify = [\n",
        "        f\"Title - {row['title']}. Abstract - {row['abstract']}\" for _, row in batch.iterrows()\n",
        "    ]\n",
        "    true_categories = batch['category'].tolist()\n",
        "\n",
        "    # Perform batch classification\n",
        "    results = classifier(texts_to_classify, candidate_labels=labels, multi_label=False)\n",
        "\n",
        "    for i, result in enumerate(results):\n",
        "        # Extract scores and labels\n",
        "        scores = result['scores']\n",
        "        predicted_label_full = result['labels'][0]\n",
        "        predicted_label = predicted_label_full.split(\" - \")[0]\n",
        "\n",
        "        # Find the index of the true category\n",
        "        true_category = true_categories[i]\n",
        "        true_index = [label.split(\" - \")[0] for label in result['labels']].index(true_category)\n",
        "\n",
        "        # Calculate statistics\n",
        "        n_labels = len(scores)\n",
        "        random_baseline = 1 / n_labels\n",
        "        normalized_scores = [score / sum(scores) for score in scores]\n",
        "        correct_score = normalized_scores[true_index]\n",
        "        gap_random = correct_score - random_baseline\n",
        "        rank = sorted(normalized_scores, reverse=True).index(correct_score) + 1\n",
        "        weighted_accuracy = 1.0 if rank == 1 else 0.5 if rank == 2 else 0.0\n",
        "\n",
        "        # Update statistics\n",
        "        category_stats[true_category][\"gap_sum\"] += gap_random\n",
        "        category_stats[true_category][\"accuracy_sum\"] += weighted_accuracy\n",
        "        category_stats[true_category][\"count\"] += 1\n",
        "\n",
        "        overall_gap_sum += gap_random\n",
        "        overall_accuracy_sum += weighted_accuracy\n",
        "        total_articles += 1\n",
        "\n",
        "    # Update console message\n",
        "    print(f\"\\rProcessing articles {batch_start + 1}-{batch_end}/{len(data)}...\", end=\"\")\n",
        "\n",
        "# Wrap-up calculations\n",
        "overall_gap_avg = overall_gap_sum / total_articles if total_articles else 0\n",
        "overall_accuracy_avg = overall_accuracy_sum / total_articles if total_articles else 0\n",
        "\n",
        "# Output results\n",
        "print(\"\\n\\nProcessing complete!\")\n",
        "print(f\"Processed Articles: {total_articles}\")\n",
        "print(f\"Overall Normalized Confidence Gap Average: {overall_gap_avg:.4f}\")\n",
        "print(f\"Overall Weighted Accuracy Average: {overall_accuracy_avg:.4f}\")\n",
        "\n",
        "for category, stats in category_stats.items():\n",
        "    count = stats[\"count\"]\n",
        "    gap_avg = stats[\"gap_sum\"] / count if count else 0\n",
        "    accuracy_avg = stats[\"accuracy_sum\"] / count if count else 0\n",
        "    print(f\"\\nCategory: {category}\")\n",
        "    print(f\"  Articles Processed: {count}\")\n",
        "    print(f\"  Average Normalized Confidence Gap: {gap_avg:.4f}\")\n",
        "    print(f\"  Average Weighted Accuracy: {accuracy_avg:.4f}\")\n"
      ],
      "metadata": {
        "id": "dYsTc73nE9dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I attempted some slight changes to the program, including shortening the labels to be more concise and changing the multi label setting to true (as it should have been from the start), but in the end saw roughly the same results just skewed out more over the different categories. With this, I switched to trying to fine-tune an existing model."
      ],
      "metadata": {
        "id": "dflzYFhGOqyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "import time\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = \"processed_articles_sanitized.csv\"  # Replace with your actual file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Define labels with descriptions\n",
        "labels = [\n",
        "    \"Biophysical - Genetics, brain activity, cellular functions.\",\n",
        "    \"Social - Relationships, cultural norms, societal dynamics.\",\n",
        "    \"Psychological - Mental processes, emotions, coping strategies.\"\n",
        "]\n",
        "\n",
        "# Load the zero-shot classification pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Variables for aggregating statistics\n",
        "category_stats = {label.split(\" - \")[0]: {\"gap_sum\": 0, \"accuracy_sum\": 0, \"count\": 0} for label in labels}\n",
        "overall_gap_sum = 0\n",
        "overall_accuracy_sum = 0\n",
        "total_articles = 0\n",
        "\n",
        "# Start timing the execution\n",
        "start_time = time.time()\n",
        "\n",
        "# Define a batch size\n",
        "batch_size = 100\n",
        "\n",
        "# Process articles in batches\n",
        "for batch_start in range(0, len(data), batch_size):\n",
        "    elapsed_time = time.time() - start_time\n",
        "    if elapsed_time > 720:\n",
        "        print(\"\\nTime limit reached. Stopping further processing...\")\n",
        "        break\n",
        "\n",
        "    batch_end = min(batch_start + batch_size, len(data))\n",
        "    batch = data.iloc[batch_start:batch_end]\n",
        "\n",
        "    # Combine titles and abstracts\n",
        "    texts_to_classify = [\n",
        "        f\"Title - {row['title']}. Abstract - {row['abstract']}\" for _, row in batch.iterrows()\n",
        "    ]\n",
        "    true_categories = batch['category'].tolist()\n",
        "\n",
        "    # Perform batch classification\n",
        "    results = classifier(texts_to_classify, candidate_labels=labels, multi_label=True)\n",
        "\n",
        "    for i, result in enumerate(results):\n",
        "        # Extract scores and labels\n",
        "        scores = result['scores']\n",
        "        predicted_label_full = result['labels'][0]\n",
        "        predicted_label = predicted_label_full.split(\" - \")[0]\n",
        "\n",
        "        # Find the index of the true category\n",
        "        true_category = true_categories[i]\n",
        "        true_index = [label.split(\" - \")[0] for label in result['labels']].index(true_category)\n",
        "\n",
        "        # Calculate statistics\n",
        "        n_labels = len(scores)\n",
        "        random_baseline = 1 / n_labels\n",
        "        normalized_scores = [score / sum(scores) for score in scores]\n",
        "        correct_score = normalized_scores[true_index]\n",
        "        gap_random = correct_score - random_baseline\n",
        "        rank = sorted(normalized_scores, reverse=True).index(correct_score) + 1\n",
        "        weighted_accuracy = 1.0 if rank == 1 else 0.5 if rank == 2 else 0.0\n",
        "\n",
        "        # Update statistics\n",
        "        category_stats[true_category][\"gap_sum\"] += gap_random\n",
        "        category_stats[true_category][\"accuracy_sum\"] += weighted_accuracy\n",
        "        category_stats[true_category][\"count\"] += 1\n",
        "\n",
        "        overall_gap_sum += gap_random\n",
        "        overall_accuracy_sum += weighted_accuracy\n",
        "        total_articles += 1\n",
        "\n",
        "    # Update console message\n",
        "    print(f\"\\rProcessing articles {batch_start + 1}-{batch_end}/{len(data)}...\", end=\"\")\n",
        "\n",
        "# Wrap-up calculations\n",
        "overall_gap_avg = overall_gap_sum / total_articles if total_articles else 0\n",
        "overall_accuracy_avg = overall_accuracy_sum / total_articles if total_articles else 0\n",
        "\n",
        "# Output results\n",
        "print(\"\\n\\nProcessing complete!\")\n",
        "print(f\"Processed Articles: {total_articles}\")\n",
        "print(f\"Overall Normalized Confidence Gap Average: {overall_gap_avg:.4f}\")\n",
        "print(f\"Overall Weighted Accuracy Average: {overall_accuracy_avg:.4f}\")\n",
        "\n",
        "for category, stats in category_stats.items():\n",
        "    count = stats[\"count\"]\n",
        "    gap_avg = stats[\"gap_sum\"] / count if count else 0\n",
        "    accuracy_avg = stats[\"accuracy_sum\"] / count if count else 0\n",
        "    print(f\"\\nCategory: {category}\")\n",
        "    print(f\"  Articles Processed: {count}\")\n",
        "    print(f\"  Average Normalized Confidence Gap: {gap_avg:.4f}\")\n",
        "    print(f\"  Average Weighted Accuracy: {accuracy_avg:.4f}\")"
      ],
      "metadata": {
        "id": "29HWVuRAOyQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning Attempt\n"
      ],
      "metadata": {
        "id": "NVyTp-AhT74X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers datasets\n"
      ],
      "metadata": {
        "id": "LE58fGdxUR2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "O_qWLCeqUZtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import get_scheduler\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "YRSnLl73UT7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset and inspect the first few rows to understand its structure\n",
        "file_path = \"processed_articles_sanitized.csv\"  # Replace with your dataset path\n",
        "dataset = load_dataset(\"csv\", data_files=file_path)\n",
        "\n",
        "# Combine title and abstract into one column\n",
        "def preprocess_function(example):\n",
        "    # Combine the 'title' and 'abstract' for a single example\n",
        "    example[\"text\"] = f\"Title: {example['title']}. Abstract: {example['abstract']}\"\n",
        "    return example\n",
        "\n",
        "# Apply preprocessing\n",
        "dataset = dataset.map(preprocess_function)\n",
        "\n",
        "# Now rename the column and drop the originals\n",
        "dataset = dataset.rename_column(\"category\", \"label\")  # Ensure 'label' matches Hugging Face expectations\n",
        "dataset = dataset.remove_columns([\"title\", \"abstract\", \"Key\"])  # Remove the original columns to keep 'text' and 'label'\n",
        "\n",
        "# Split the dataset into training and validation sets (80% train, 20% validation)\n",
        "train_data = dataset[\"train\"].train_test_split(test_size=0.2)[\"train\"]\n",
        "val_data = dataset[\"train\"].train_test_split(test_size=0.2)[\"test\"]\n",
        "\n",
        "# Print one example entry from the dataset to verify\n",
        "print(\"One example entry from the dataset:\")\n",
        "print(train_data[0])  # Print the first entry of the train dataset to verify\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "29Qzrar2UkIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"  # Ensure W&B is fully disabled\n",
        "os.environ[\"WANDB_MODE\"] = \"dryrun\"\n",
        "\n",
        "print(train_data[0])\n",
        "\n",
        "# Initialize tokenizer and pre-trained model\n",
        "model_name = \"bert-base-uncased\"  # Replace with your chosen model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # Ensure the \"text\" is a list of strings, if not, join them into a single string\n",
        "    texts = [str(text) for text in examples[\"text\"]]  # Make sure they are strings\n",
        "    return tokenizer(texts, padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_train = train_data.map(tokenize_function, batched=True)\n",
        "tokenized_val = val_data.map(tokenize_function, batched=True)\n",
        "\n",
        "label_map = {\n",
        "    \"Biophysical\": 0,\n",
        "    \"Psychological\": 1,\n",
        "    \"Social\": 2\n",
        "}\n",
        "\n",
        "def flatten_and_convert_labels(examples):\n",
        "    # If the label is a list, get the first element\n",
        "    if isinstance(examples['label'], list):\n",
        "        examples['label'] = examples['label'][0]  # Extract the first element if it's a list\n",
        "\n",
        "    # Convert the label from string to integer based on the label_map\n",
        "    examples['label'] = label_map.get(examples['label'], examples['label'])  # Convert label to integer\n",
        "\n",
        "    return examples\n",
        "\n",
        "train_data = train_data.map(flatten_and_convert_labels)\n",
        "val_data = val_data.map(flatten_and_convert_labels)\n",
        "\n",
        "print(train_data[0])\n",
        "\n",
        "# Load the model for sequence classification\n",
        "num_labels = 3  # For Biophysical, Psychological, Social\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",  # Directory to save model/checkpoints\n",
        "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
        "    learning_rate=3e-5,  # Standard learning rate for fine-tuning\n",
        "    per_device_train_batch_size=8,  # Adjust as needed for memory\n",
        "    num_train_epochs=5,  # Experiment with 2–5 epochs\n",
        "    weight_decay=0.01,  # Regularization to avoid overfitting\n",
        "    save_total_limit=2,  # Keep only the last two checkpoints\n",
        "    logging_dir=\"./logs\",  # Logging directory\n",
        "    logging_steps=10,  # Log training progress every 10 steps\n",
        "    report_to=None,  # Disable W&B logging\n",
        "    lr_scheduler_type=\"linear\",  # Linearly decrease learning rate\n",
        ")\n",
        "\n",
        "# Set up the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "trainer.save_model(\"fine_tuned_model\")\n",
        "\n",
        "# Evaluate and print results\n",
        "results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", results)\n"
      ],
      "metadata": {
        "id": "hMNQKPWEUsbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Make predictions on your validation/test set\n",
        "predictions = trainer.predict(tokenized_val)\n",
        "predicted_labels = predictions.predictions.argmax(axis=1)\n",
        "\n",
        "# Compare with the true labels\n",
        "true_labels = tokenized_val[\"label\"]\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "pIrFJ8U3t8ez"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}